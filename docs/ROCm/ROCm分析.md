相比英伟达的TensorRT-LLM优化框架，AMD的ROCm生态仍显单薄。例如运行Llama 3-70B时，7900 XTX性能骤降40%，暴露出对复杂模型支持的短板。例如运行Llama 3-70B时，7900 XTX性能骤降40%，暴露出对复杂模型支持的短板。

ollama, 偏向个人用户，部署简单网络高效。主要在cpu上运行大模型，通过将部分参数卸载到gpu来提升运行速度。
	[Ollama迎来重大更新，引入flash attention修复和KV cache量化](https://zhuanlan.zhihu.com/p/10945639268)
sg-lang/vllm，偏向商用，并发性带宽高。主要在gpu上运行大模型，如果offload技术卸载部分参数到cpu进行执行。性能提升在kvcache的优化上。

|       |            |                       |      |
| ----- | ---------- | --------------------- | ---- |
|       | croe build |                       |      |
| 22226 | 2116345    | 5.14.0-556.el9.x86_64 | fail |
| 22016 | 2109964    | 5.14.0-547.el9.x86_64 | pass |
